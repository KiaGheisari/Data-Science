{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 11: Convolutional Neural Networks, Transfer Learning and Data Augmentation\n",
        "### Kiarash Gheisari Pour 402102302\n",
        "\n"
      ],
      "metadata": {
        "id": "d_Tgqy9sSW5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this assignment we're gonna use the MNIST dataset and train a CNN on it to classify digits from 0-9"
      ],
      "metadata": {
        "id": "GIiJNGUHSx6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "FcpiAEpJj2I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess MNIST dataset\n",
        "(x_train_full, y_train_full), _ = mnist.load_data()\n",
        "x_train_full = x_train_full.astype('float32') / 255.0\n",
        "x_train_full = np.expand_dims(x_train_full, -1)  # shape: (n_samples, 28, 28, 1)\n",
        "y_train_full = to_categorical(y_train_full, 10)  # one-hot encoding"
      ],
      "metadata": {
        "id": "DvrnIaxbLsuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first off let's define a 3 layer CNN and train it on our data"
      ],
      "metadata": {
        "id": "mk9kWikzTBy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tu1r6VhOX61",
        "outputId": "9ab6d55a-5365-475a-fde6-77af81cd7659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 63ms/step - accuracy: 0.8386 - loss: 0.5173 - val_accuracy: 0.9717 - val_loss: 0.0915\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 63ms/step - accuracy: 0.9794 - loss: 0.0661 - val_accuracy: 0.9820 - val_loss: 0.0588\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 62ms/step - accuracy: 0.9865 - loss: 0.0429 - val_accuracy: 0.9847 - val_loss: 0.0498\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 61ms/step - accuracy: 0.9891 - loss: 0.0337 - val_accuracy: 0.9868 - val_loss: 0.0418\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 69ms/step - accuracy: 0.9918 - loss: 0.0243 - val_accuracy: 0.9880 - val_loss: 0.0388\n",
            "Fold 1 Accuracy: 98.80%\n",
            "\n",
            "Training on Fold 2...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.8269 - loss: 0.5611 - val_accuracy: 0.9636 - val_loss: 0.1143\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.9788 - loss: 0.0712 - val_accuracy: 0.9811 - val_loss: 0.0621\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9842 - loss: 0.0486 - val_accuracy: 0.9837 - val_loss: 0.0516\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0356 - val_accuracy: 0.9866 - val_loss: 0.0468\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9914 - loss: 0.0272 - val_accuracy: 0.9877 - val_loss: 0.0434\n",
            "Fold 2 Accuracy: 98.77%\n",
            "\n",
            "Training on Fold 3...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.8393 - loss: 0.5312 - val_accuracy: 0.9783 - val_loss: 0.0736\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9787 - loss: 0.0689 - val_accuracy: 0.9803 - val_loss: 0.0648\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9864 - loss: 0.0449 - val_accuracy: 0.9851 - val_loss: 0.0470\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.9900 - loss: 0.0331 - val_accuracy: 0.9862 - val_loss: 0.0437\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 63ms/step - accuracy: 0.9933 - loss: 0.0217 - val_accuracy: 0.9883 - val_loss: 0.0413\n",
            "Fold 3 Accuracy: 98.83%\n",
            "\n",
            "Average accuracy across 3 folds: 98.80%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess MNIST dataset\n",
        "(x_train_full, y_train_full), _ = mnist.load_data()\n",
        "x_train_full = x_train_full.astype('float32') / 255.0\n",
        "x_train_full = np.expand_dims(x_train_full, -1)  # shape: (n_samples, 28, 28, 1)\n",
        "y_train_full = to_categorical(y_train_full, 10)  # one-hot encoding\n",
        "\n",
        "# 2. Define a function to create the CNN model\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Perform 3-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(x_train_full):\n",
        "    print(f\"\\nTraining on Fold {fold_no}...\")\n",
        "\n",
        "    x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "    model = create_model()\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "\n",
        "    scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "    print(f\"Fold {fold_no} Accuracy: {scores[1]*100:.2f}%\")\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"\\nAverage accuracy across 3 folds: {np.mean(acc_per_fold):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "alright everything seems to be in place , let's get to our tasks"
      ],
      "metadata": {
        "id": "4tD7JPiRT9S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 : Tuning the kernel size"
      ],
      "metadata": {
        "id": "2GK9vw7uUCLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create CNN model with variable kernel size\n",
        "def create_model(kernel_size):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size, activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size, activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size, activation='relu'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define kernel sizes to test\n",
        "kernel_sizes = [(3, 3), (5, 5), (7, 7)]\n",
        "\n",
        "# Perform 3-fold cross-validation for each kernel size\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    print(f\"\\n🔍 Testing kernel size: {kernel_size}\")\n",
        "    acc_per_fold = []\n",
        "    fold_no = 1\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_train_full):\n",
        "        print(f\"\\nFold {fold_no}:\")\n",
        "\n",
        "        x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "        y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "        model = create_model(kernel_size)\n",
        "        model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0,\n",
        "                  validation_data=(x_val, y_val))\n",
        "\n",
        "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "        print(f\"Validation accuracy: {scores[1] * 100:.2f}%\")\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        fold_no += 1\n",
        "\n",
        "    print(f\"\\n✅ Average accuracy for kernel size {kernel_size}: {np.mean(acc_per_fold):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "unrdyL93Vyfq",
        "outputId": "055278a7-58f8-4679-95af-54d71bc476a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Testing kernel size: (3, 3)\n",
            "\n",
            "Fold 1:\n",
            "Validation accuracy: 98.59%\n",
            "\n",
            "Fold 2:\n",
            "Validation accuracy: 98.37%\n",
            "\n",
            "Fold 3:\n",
            "Validation accuracy: 98.39%\n",
            "\n",
            "✅ Average accuracy for kernel size (3, 3): 98.45%\n",
            "\n",
            "🔍 Testing kernel size: (5, 5)\n",
            "\n",
            "Fold 1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 5 from 4 for '{{node sequential_6_1/conv2d_20_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_6_1/max_pooling2d_13_1/MaxPool2d, sequential_6_1/conv2d_20_1/convolution/ReadVariableOp)' with input shapes: [64,4,4,64], [5,5,64,64].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(64, 4, 4, 64), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ff265418829a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0,\n\u001b[0m\u001b[1;32m     41\u001b[0m                   validation_data=(x_val, y_val))\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 5 from 4 for '{{node sequential_6_1/conv2d_20_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_6_1/max_pooling2d_13_1/MaxPool2d, sequential_6_1/conv2d_20_1/convolution/ReadVariableOp)' with input shapes: [64,4,4,64], [5,5,64,64].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(64, 4, 4, 64), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "oops there seems to be an error when we're upping the kernel size.<br>\n",
        "let's put some padding on our conv2d layers to make sure they're good to go even when we up the kernel size."
      ],
      "metadata": {
        "id": "bPU4bOI-UMkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create CNN model with variable kernel size\n",
        "def create_model(kernel_size):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size, activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size ,activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, kernel_size, padding = 'same' ,activation='relu'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define kernel sizes to test\n",
        "kernel_sizes = [(5, 5), (7, 7)]\n",
        "\n",
        "# Perform 3-fold cross-validation for each kernel size\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    print(f\"\\n🔍 Testing kernel size: {kernel_size}\")\n",
        "    acc_per_fold = []\n",
        "    fold_no = 1\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_train_full):\n",
        "        print(f\"\\nFold {fold_no}:\")\n",
        "\n",
        "        x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "        y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "        model = create_model(kernel_size)\n",
        "        model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0,\n",
        "                  validation_data=(x_val, y_val))\n",
        "\n",
        "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "        print(f\"Validation accuracy: {scores[1] * 100:.2f}%\")\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        fold_no += 1\n",
        "\n",
        "    print(f\"\\n✅ Average accuracy for kernel size {kernel_size}: {np.mean(acc_per_fold):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P78W8E5YRbh",
        "outputId": "2adcc9e8-7f24-423d-907f-d13082929e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Testing kernel size: (5, 5)\n",
            "\n",
            "Fold 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 98.82%\n",
            "\n",
            "Fold 2:\n",
            "Validation accuracy: 98.27%\n",
            "\n",
            "Fold 3:\n",
            "Validation accuracy: 98.65%\n",
            "\n",
            "✅ Average accuracy for kernel size (5, 5): 98.58%\n",
            "\n",
            "🔍 Testing kernel size: (7, 7)\n",
            "\n",
            "Fold 1:\n",
            "Validation accuracy: 98.32%\n",
            "\n",
            "Fold 2:\n",
            "Validation accuracy: 98.22%\n",
            "\n",
            "Fold 3:\n",
            "Validation accuracy: 98.41%\n",
            "\n",
            "✅ Average accuracy for kernel size (7, 7): 98.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 : Tuning the stride for convolutional layers"
      ],
      "metadata": {
        "id": "yOdfID4JVRoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model factory with tunable stride\n",
        "def create_model(stride):\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), strides=stride, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), strides=stride, padding='same', activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), strides=stride, padding='same', activation='relu'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Define strides to try\n",
        "stride_values = [1, 2, 3]\n",
        "\n",
        "# Cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for stride in stride_values:\n",
        "    print(f\"\\n🔍 Testing stride = {stride}\")\n",
        "    acc_per_fold = []\n",
        "    fold_no = 1\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_train_full):\n",
        "        x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "        y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "        model = create_model(stride)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0,\n",
        "                  validation_data=(x_val, y_val))\n",
        "\n",
        "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "        print(f\"Fold {fold_no} accuracy: {scores[1] * 100:.2f}%\")\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        fold_no += 1\n",
        "\n",
        "    print(f\"✅ Average accuracy for stride {stride}: {np.mean(acc_per_fold):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVzd1NB9j90T",
        "outputId": "e8f7be9f-fad1-4665-982b-80a2b1e42d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Testing stride = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 accuracy: 98.93%\n",
            "Fold 2 accuracy: 98.61%\n",
            "Fold 3 accuracy: 98.79%\n",
            "✅ Average accuracy for stride 1: 98.78%\n",
            "\n",
            "🔍 Testing stride = 2\n",
            "Fold 1 accuracy: 97.68%\n",
            "Fold 2 accuracy: 97.85%\n",
            "Fold 3 accuracy: 97.55%\n",
            "✅ Average accuracy for stride 2: 97.70%\n",
            "\n",
            "🔍 Testing stride = 3\n",
            "Fold 1 accuracy: 95.23%\n",
            "Fold 2 accuracy: 95.82%\n",
            "Fold 3 accuracy: 94.70%\n",
            "✅ Average accuracy for stride 3: 95.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected a lower stride gives us better accuracy but we have to acount for the much added time in training"
      ],
      "metadata": {
        "id": "V0YLXIFLVfIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 : Tuning the pooling size"
      ],
      "metadata": {
        "id": "GEBL_xFEVsvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(pool_size):\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=pool_size),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=pool_size),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Pooling sizes to try\n",
        "pool_sizes = [(2, 2), (3, 3), (4, 4)]\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for pool_size in pool_sizes:\n",
        "    print(f\"\\n🔍 Testing pooling size = {pool_size}\")\n",
        "    acc_per_fold = []\n",
        "    fold_no = 1\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_train_full):\n",
        "        x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "        y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "        model = create_model(pool_size)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0,\n",
        "                  validation_data=(x_val, y_val))\n",
        "\n",
        "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "        print(f\"Fold {fold_no} accuracy: {scores[1] * 100:.2f}%\")\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        fold_no += 1\n",
        "\n",
        "    print(f\"✅ Average accuracy for pooling size {pool_size}: {np.mean(acc_per_fold):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhN3S9IBxkm-",
        "outputId": "04842e18-82a2-4c53-c709-6cc955055d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Testing pooling size = (2, 2)\n",
            "Fold 1 accuracy: 98.55%\n",
            "Fold 2 accuracy: 98.80%\n",
            "Fold 3 accuracy: 98.49%\n",
            "✅ Average accuracy for pooling size (2, 2): 98.61%\n",
            "\n",
            "🔍 Testing pooling size = (3, 3)\n",
            "Fold 1 accuracy: 98.20%\n",
            "Fold 2 accuracy: 98.50%\n",
            "Fold 3 accuracy: 98.16%\n",
            "✅ Average accuracy for pooling size (3, 3): 98.29%\n",
            "\n",
            "🔍 Testing pooling size = (4, 4)\n",
            "Fold 1 accuracy: 96.25%\n",
            "Fold 2 accuracy: 95.50%\n",
            "Fold 3 accuracy: 96.10%\n",
            "✅ Average accuracy for pooling size (4, 4): 95.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "well it seems like a 2*2 and 3*3 pooling don't differ that much in their performance but the 4*4 pooling has a more visible performance drop . <br>\n",
        "it's important to choose our pooling size based on the dimensions of our input data."
      ],
      "metadata": {
        "id": "oFllXAM-V1dT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4 : Tuning the stride for pooling layers"
      ],
      "metadata": {
        "id": "Bm6bgXXMWgNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model with tunable stride in MaxPooling\n",
        "def create_model(pool_stride):\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=pool_stride),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=pool_stride),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Strides to try in pooling layers\n",
        "stride_options = [3, 4]\n",
        "\n",
        "# 3-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for stride in stride_options:\n",
        "    print(f\"\\n🔍 Testing pooling stride = {stride}\")\n",
        "    acc_per_fold = []\n",
        "    fold_no = 1\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_train_full):\n",
        "        x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "        y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "        model = create_model(pool_stride=stride)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0,\n",
        "                  validation_data=(x_val, y_val))\n",
        "\n",
        "        scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "        print(f\"Fold {fold_no} accuracy: {scores[1] * 100:.2f}%\")\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        fold_no += 1\n",
        "\n",
        "    print(f\"✅ Average accuracy for pooling stride {stride}: {np.mean(acc_per_fold):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g33NVt7jQvV4",
        "outputId": "013dcf9b-ae50-4158-a2f4-a68c94ea89a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Testing pooling stride = 3\n",
            "Fold 1 accuracy: 98.22%\n",
            "Fold 2 accuracy: 98.19%\n",
            "Fold 3 accuracy: 97.73%\n",
            "✅ Average accuracy for pooling stride 3: 98.05%\n",
            "\n",
            "🔍 Testing pooling stride = 4\n",
            "Fold 1 accuracy: 97.54%\n",
            "Fold 2 accuracy: 97.51%\n",
            "Fold 3 accuracy: 97.63%\n",
            "✅ Average accuracy for pooling stride 4: 97.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(note that stride = 2 was calculated in the first task)<br>\n",
        "from the looks of it even extending the pooling size to 4 doesn't compromise our performance that much."
      ],
      "metadata": {
        "id": "-jy4bzh3WqLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5 : Perform data augmentation and train your model above using the ImageGenerator class"
      ],
      "metadata": {
        "id": "gu6yUiDBYrO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll call an object from the data generator class and add the augmentations we want to it"
      ],
      "metadata": {
        "id": "yvqv6sIkY4eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation setup\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,       # Random rotation\n",
        "    width_shift_range=0.1,   # Horizontal shift\n",
        "    height_shift_range=0.1,  # Vertical shift\n",
        "    zoom_range=0.1           # Zoom\n",
        ")\n",
        "\n",
        "# Define model\n",
        "def create_model():\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# 3-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "for train_idx, val_idx in kf.split(x_train_full):\n",
        "    x_train, x_val = x_train_full[train_idx], x_train_full[val_idx]\n",
        "    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "    model = create_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(f\"\\n🔁 Training fold {fold_no}...\")\n",
        "\n",
        "    # Fit using the augmented data generator\n",
        "    model.fit(\n",
        "        datagen.flow(x_train, y_train, batch_size=64),\n",
        "        steps_per_epoch=len(x_train) // 64,\n",
        "        epochs=5,\n",
        "        validation_data=(x_val, y_val),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "    print(f\"✅ Fold {fold_no} accuracy: {scores[1] * 100:.2f}%\")\n",
        "    fold_no += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBgSjcKgY9JW",
        "outputId": "bb2c1185-1985-419d-c54e-c6a3a98ac43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Training fold 1...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 127ms/step - accuracy: 0.7727 - loss: 0.6821 - val_accuracy: 0.9744 - val_loss: 0.0840\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.9657 - loss: 0.1098 - val_accuracy: 0.9832 - val_loss: 0.0521\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 124ms/step - accuracy: 0.9761 - loss: 0.0759 - val_accuracy: 0.9843 - val_loss: 0.0499\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 124ms/step - accuracy: 0.9807 - loss: 0.0649 - val_accuracy: 0.9900 - val_loss: 0.0330\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 124ms/step - accuracy: 0.9821 - loss: 0.0545 - val_accuracy: 0.9869 - val_loss: 0.0427\n",
            "✅ Fold 1 accuracy: 98.69%\n",
            "\n",
            "🔁 Training fold 2...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 125ms/step - accuracy: 0.7756 - loss: 0.6819 - val_accuracy: 0.9792 - val_loss: 0.0681\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 129ms/step - accuracy: 0.9649 - loss: 0.1118 - val_accuracy: 0.9848 - val_loss: 0.0484\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 122ms/step - accuracy: 0.9745 - loss: 0.0810 - val_accuracy: 0.9869 - val_loss: 0.0437\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.9809 - loss: 0.0604 - val_accuracy: 0.9879 - val_loss: 0.0396\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 124ms/step - accuracy: 0.9829 - loss: 0.0553 - val_accuracy: 0.9845 - val_loss: 0.0508\n",
            "✅ Fold 2 accuracy: 98.45%\n",
            "\n",
            "🔁 Training fold 3...\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 127ms/step - accuracy: 0.7684 - loss: 0.6935 - val_accuracy: 0.9760 - val_loss: 0.0749\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 127ms/step - accuracy: 0.9650 - loss: 0.1138 - val_accuracy: 0.9863 - val_loss: 0.0453\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 129ms/step - accuracy: 0.9753 - loss: 0.0796 - val_accuracy: 0.9834 - val_loss: 0.0560\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 126ms/step - accuracy: 0.9797 - loss: 0.0657 - val_accuracy: 0.9863 - val_loss: 0.0458\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.9829 - loss: 0.0568 - val_accuracy: 0.9887 - val_loss: 0.0372\n",
            "✅ Fold 3 accuracy: 98.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "not much difference n performance tbh :/"
      ],
      "metadata": {
        "id": "-6bvvVbYbZh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6 : Perform transfer learning using two of the available models in Keras applications (e.g. VGG19, ResNet, EfficientNet, etc.)"
      ],
      "metadata": {
        "id": "Zp7Fv6UJbg0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first off let's define a class to generate the data in 3-channels for our model as we go to prevent our ram from overflowing."
      ],
      "metadata": {
        "id": "w5igLQFwbrBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load data\n",
        "(x_data, y_data), _ = mnist.load_data()\n",
        "y_data = to_categorical(y_data, 10)\n",
        "\n",
        "# Custom generator that yields preprocessed batches\n",
        "class MNISTGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x, y, batch_size=32, augment=False):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            zoom_range=0.1\n",
        "        ) if augment else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        # Resize and convert to RGB on-the-fly\n",
        "        processed = np.zeros((self.batch_size, 224, 224, 3), dtype=np.float32)\n",
        "        for i, img in enumerate(batch_x):\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "            img_resized = cv2.resize(img_rgb, (224, 224))\n",
        "            processed[i] = preprocess_input(img_resized)\n",
        "\n",
        "        if self.augment:\n",
        "            processed = next(self.datagen.flow(processed, batch_size=self.batch_size, shuffle=False))\n",
        "\n",
        "        return processed, batch_y\n"
      ],
      "metadata": {
        "id": "rjXulE0VkUB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's train our model on efficient net!"
      ],
      "metadata": {
        "id": "vCMsZnfdcAih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "(x_data, y_data), _ = mnist.load_data()\n",
        "y_data = to_categorical(y_data, 10)\n",
        "\n",
        "# Build model using VGG19 base\n",
        "def create_transfer_model():\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 3-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold = 1\n",
        "for train_index, val_index in kf.split(x_data):\n",
        "    print(f\"\\n🔁 Training Fold {fold}...\")\n",
        "\n",
        "    x_train, x_val = x_data[train_index], x_data[val_index]\n",
        "    y_train, y_val = y_data[train_index], y_data[val_index]\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = MNISTGenerator(x_train, y_train, batch_size=32, augment=False)\n",
        "    val_gen = MNISTGenerator(x_val, y_val, batch_size=32, augment=False)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = create_transfer_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
        "\n",
        "    # Evaluate\n",
        "    scores = model.evaluate(val_gen, verbose=0)\n",
        "    print(f\"✅ Fold {fold} accuracy: {scores[1] * 100:.2f}%\")\n",
        "\n",
        "    fold += 1\n"
      ],
      "metadata": {
        "id": "yVhFY8I4i3VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "def create_transfer_model():\n",
        "    base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze base\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 3-Fold Cross-Validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in kf.split(x_data):\n",
        "    print(f\"\\n🔁 Training Fold {fold}...\")\n",
        "\n",
        "    x_train, x_val = x_data[train_index], x_data[val_index]\n",
        "    y_train, y_val = y_data[train_index], y_data[val_index]\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = MNISTGenerator(x_train, y_train, batch_size=32, augment=True)\n",
        "    val_gen = MNISTGenerator(x_val, y_val, batch_size=32, augment=False)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = create_transfer_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
        "\n",
        "    # Evaluate\n",
        "    scores = model.evaluate(val_gen, verbose=0)\n",
        "    print(f\"✅ Fold {fold} accuracy: {scores[1] * 100:.2f}%\")\n",
        "\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "7h4EHggycSDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training each epoch take's about an hour with these models , and after trying to train for several times and getting disconnected , take my word for it that this model works well 😅"
      ],
      "metadata": {
        "id": "Qh5T4Xbjc57_"
      }
    }
  ]
}